Assignment #1
	Weather Data Analysis with DataFrame API
	a) Load Weather Dataset and create DataFrame
	b) Show Min and Max Temperature
	c) Show month wise Min and Max Temperature

Assignment #2
	Bank Marketing Campaign Data Analysis with DataFrame API
	a) Load Bank Marketing Dataset and create DataFrame		
	b) Give marketing success rate. (No. of people subscribed / total no. of entries)
	c) Give marketing failure rate
	d) Maximum, Mean, and Minimum age of the average targeted customer
	e) Check the quality of customers by checking the average balance, median balance of customers
	f) Check if age matters in marketing subscription for deposit
	g) Show AgeGroup [Teenagers, Youngsters, MiddleAgers, Seniors] wise Subscription Count.
	h) Check if marital status mattered for subscription to deposit.
	i) Check if age and marital status together mattered for subscription to deposit scheme
	

Assignment #3

	Bank Marketing Campaign Data Analysis with PySpark SQL

	Create PySpark Application - bank-marketing-analysis.py. Perform below operations.
	a) Load Bank Marketing Campaign Data from csv file
	b) Get AgeGroup wise SubscriptionCount
	c) Write the output in parquet file format
	d) Load the data from parquet file written above
	e) Show the data
	f) Filter AgeGroup with SubcriptionCount > 2000 and write into Avro file format
	g) Load the data from avro file written above
	h) Show the data

	1] spark-submit bank-marketing-analysis.py => Runs in local mode
	2] Run in cluster mode
	3] Schedule to PySparkApplication to run every N mins

Assignment #4

	Enhance above Bank Marketing Campaign Data Analysis PySpark Application to perform the data processing
	as pipeline of four different stages viz. Loading, Validation, Transformation and Expot each as separate PySpark applicaiton.

	Create PySpark Application - bank-marketing-data-loading.py. Perform below operations.
	a) Load Bank Marketing Campaign Data csv file from local to HDFS file system under '/user/training/bankmarketing/raw'
	b) Load Bank Marketing Campaign Data from HDFS file system under '/user/training/bankmarketing/raw' and create DataFrame
	c) Convert the data into parquet format and write into HDFS file system under '/user/training/bankmarketing/staging'
	d) Data should be moved to '/user/training/bankmarketing/raw/yyyymmdd/success' once the data loading job completed successfully
	f) Data should be moved to '/user/training/bankmarketing/raw/yyyymmdd/error' once the data loading job is failed due to data error
	
	Create PySpark Application - bank-marketing-validation.py. Perform below operations.
	a) Load Bank Marketing Campaign Data in Parquet format from HDFS file system under '/user/training/bankmarketing/staging'
	b) Remove all 'unknown' job records 
	c) Replace 'unknown' contact nos with 1234567890 and 'unknown' poutcome with 'na'
	d) Write the output as Parquet format into HDFS file system under '/user/training/bankmarketing/validated'
	e) Data should be moved to '/user/training/bankmarketing/staging/yyyymmdd/success' once the validation job completed successfully
	f) Data should be moved to '/user/training/bankmarketing/staging/yyyymmdd/error' once the validation job is failed due to data error

	Create PySpark Application - bank-marketing-tranformation.py. Perform below operations.
	a) Load validated Bank Marketing Campaign Data in Parquet format from HDFS file system under '/user/training/bankmarketing/validated'
	b) Get AgeGroup wise SubscriptionCount
	c) Filter AgeGroup with SubcriptionCount > 2000 
	d) Write the output as Avro format into HDFS file system under '/user/training/bankmarketing/processed'
	e) Data should be moved to '/user/training/bankmarketing/validated/yyyymmdd/success' once the trasnfomation job completed successfully
	f) Data should be moved to '/user/training/bankmarketing/validated/yyyymmdd/error' once the transformation job is failed
	
	Create PySpark Application - bank-marketing-export.py. Perform below operations.
	a) Load processed Bank Marketing Campaign Data in Avro format from HDFS file system under '/user/training/bankmarketing/processed'
	b) Export the data into RDBMS (MySQL DB) under bankmaketing schema and subcription_count table
	c) Data should be moved to '/user/training/bankmarketing/processed/yyyymmdd/success' once the export job completed successfully
	d) Data should be moved to '/user/training/bankmarketing/processed/yyyymmdd/error' once the export job is failed

	Create Shell Script - bank-marketing-workflow.sh. Performa below operations.
	a) Create a workflow to sequentially execute Data Loading, Validation, Tranformation and Export jobs
	b) Schedule to run this workflow every N mins e.g. 15 mins and process the new input dataset if any







