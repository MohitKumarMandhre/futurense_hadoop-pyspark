		SPARK
			- Distributed in-memory data processing framework
			- Spark can support Local, HDFS, S3, Azure Blob, Azure Data Lake FS, Cassandra, etc.
			- Multi Language support - Scala, Java, Python, SQL and R
			
			- MR vs Spark

			- Spark Modes
				- Local
				- Standalone - uses it’s own cluster manager - master / workers
				- YARN
				- Mesos
				- K8s

			- Spark Architecture
				- Driver
					- Spark Context
				- Cluster Manager
				- Worker

			- Spark Setup
				STEP 1: Download and Setup Spark
				STEP 2: Configure Spark to enable event logs
				STEP 3: Launch PySpark shell

		 	- Spark Modules
				- Spark Core
					- Spark Context - Coordinated job execution, cluster connection details, DAG
					- Spark Session - Abstraction on different contexts - Spark Context, SQL Context, Hive Context, Streaming Context
					- RDD
						- Distributed, Immutable, Partitioned, Lazy Evaluated, Fault Tolerant
						- Diff ways to create RDD
							- Parallize the collection
							- Loading Data from External File
					- DAG (Directe Acyclic Graph)
					- Transformation
						- Narrow -> Map, Filter, FlatMap, etc..
						- Wide - Repartition, Join, GroupByKey, ReduceByKey, AggregateByKey, Collaesce, etc..
					- Action
						- Count, Sum, Collect, Reduce, ForEach, etc..		

					[1,2,3,4,5] -> 15
					[(2013, [1,1,1]),(2020,[1,1,1,1,])] -> [(2013, 3), (2020, 4)] 
					
				- Spark SQL
				- Spark Streaming
				- Spark MLib
				- Spring GraphX
			

			Transformations
				- Map
				- FlatMap
				- Filter
				- mapPartitions

				- repartition
				- coalesce
				- distinct

				- union
				- intersection
				
				- join
				- cogroup
				- cartesian

				- groupByKey
				- reduceByKey
				- aggregateByKey
				- sortByKey

			Actions
				- count
				- collect
				- reduce
				- sum
				- min
				- max
				- mean
				- forEach
				- saveAsTextFile
				- first
				- take
				- takeSample
				- countByKey
		
			Persistence Level

				- MEMORY_ONLY
				- DISK_ONLY
				- MEMORY_AND_DISK
				- MEMORY_ONLY_2
				- DISK_ONLY_2
				- MEMORY_AND_DISK_2
				
			Share Variables
				Broadcast Variable
				Accumulator

			
	Spark SQL

			Data Frame
			
				- Create DataFram
					- DataFrameReader -> spark.read() -> Load data from external source
					- From RDD -> rdd.toDF()
					- Using CreateDataFrame with RDD -> spark.createDataFrame(rdd)
					- Using CreateDataFrame with List of Rows -> spark.createDataFrame([Row(),Row()]
					- Using CreateDataFrame with List of Tuples -> spark.createDataFrame([(),()]
					- Using CreateDataFrame with Pandas DF -> spark.createDataFrame(pandasDF)
					- Using CreateDataFrame with explicit schema -> spark.createDataFrame(rdd,schema=list|string|structtype)

				- Print Schema -> df.printSchema()
				- List values -> df.show()
				
				- Select columns
					- df.select(df.col, df[‘col’], “col”)


				- Functions
					- String Functions -contcat_ws, lower, upper,
					- Math Functions
					- Date Functions
					- Window Functions
					- etc.

				- Columns
					- Add Column - df.withColumn(‘col_name’, fun())
					- Drop Column
					- Rename Column
				
				- Filtering
				- Grouping
				- Aggregation
				- Joins

				- Write Data From DF
				
				- Create Data Frame with Complex Data Types
					- Struct, Array, Map
				- When Function
				- Expr Function
				- Missing Values Handling - Filter, Drop, Replace
				- Cast Function
				- Col, Lit function
				- Flattening with Explode, Explode Outer, PosExplode, PosExplode Outer
				- Filter
				- GroupBy
				- Array Contains, to_csv()
				- Aggregate Functions
				- Where
				- Distinct
				- OrderBy

		               - Joins
					- Inner
					- Full Outer
					- Left Outer
					- Right Outer			
					- Left Semi
					- Left Anti
					- Cross
					- Self
			
				- Window Functions
					- row_number()
					- rank()
					- dense_rank()
					- ntile()
					- percent_rank()
					- cume_dist()
					- lag()
					- lead()

				- Writing SQL queries on DF
					- Register DF as Temp Table or View
					- spark.sql()

				- Read/Write Data with DF
					- spark.read => DataFrameReader
					- df.write	 => DataFrameWriter

					- spark.read.csv()		
					- spark.read.options().csv()		

					- df.write.csv()
					- spark.read().options().csv()	

				
					- spark.read.load(path, format=csv, options)
					- spark.read.format(“csv”).options().load()

					- df.write.save(path, format=csv, options)
					- df.write.format(“csv”).options().save()

		Spark Streaming
			Storm vs Kafka vs Flink vs Druid vs Spark Streaming
			DStream
				- Streaming data be converted into micro batches and each batch be processed and generates output
				- Streaming Context 
					- part of pyspark.streaming package
					- need to pass Spark Context and batch interval while creating Streaming Context
					- sources
						- textFileStream
						- socketTextStream 
					- apply transformations on DStream
					- start Streaming Context
				- Windowing
					- Sliding Window
					- Tumbling Window

				
			Structured Streaming
				- DataFrame API
				- Unbounded table
				- spark.readstream.format(“socket”).load()
				- apply transformations on streaming DataFrame
				- spark.writestream.outputMode(“complete|append|update”).format(“console|file”).start()

				- Windowing
				- Watermarking


		Kafka
			- Communication Styles - Sync vs Async
			- Messaging Semantics
				- Queue - point to point communication
				- Topic 	- pub sub / one to many

			- Kafka Architecture
				- Producer - produces the events
				- Broker	    - stores and process the events
				- Consumer - consumes the events
				- Zookeeper - co-ordination service and stores metadata

		 	- Kafka Concepts
				- Message/Event
				- Topic
				- Partition
					- Leader
					- Follower / ISR
				- Replication

				- Consumer Group

				- Offset

				- Create topic with 3 partitions on 3 broker Kafka cluster - partitions per broker?
			

			- Structured Streaming Kafka integration
				- create Kafka streaming source and produce streaming DF
					- spark.readStream
							.format(“kafka”)
								.option(“kafka.bootstrap.servers”, ”localhost:9092”)
								.option(“subscribe”, ”movie-events”)
								.option(“group.id”, ”movie-events-group”)

							.load()
				- apply transformations on streaming DF
				- write streaming DF output into Kafka topics
					- spark.writeStream
							.outputMode(“complete|append|update”)
							.format(“kafka”)
								.option(“kafka.bootstrap.servers”, ”localhost:9092”)
								.option(“topic”, “movie-events-out’)
							.start()

				
