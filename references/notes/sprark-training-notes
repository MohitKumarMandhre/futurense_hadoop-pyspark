		SPARK
			- Distributed in-memory data processing framework
			- Spark can support Local, HDFS, S3, Azure Blob, Azure Data Lake FS, Cassandra, etc.
			- Multi Language support - Scala, Java, Python, SQL and R
			
			- MR vs Spark

			- Spark Modes
				- Local
				- Standalone - uses it’s own cluster manager - master / workers
				- YARN
				- Mesos
				- K8s

			- Spark Architecture
				- Driver
					- Spark Context
				- Cluster Manager
				- Worker

			- Spark Setup
				STEP 1: Download and Setup Spark
				STEP 2: Configure Spark to enable event logs
				STEP 3: Launch PySpark shell

		 	- Spark Modules
				- Spark Core
					- Spark Context - Coordinated job execution, cluster connection details, DAG
					- Spark Session - Abstraction on different contexts - Spark Context, SQL Context, Hive Context, Streaming Context
					- RDD
						- Distributed, Immutable, Partitioned, Lazy Evaluated, Fault Tolerant
						- Diff ways to create RDD
							- Parallize the collection
							- Loading Data from External File
					- DAG (Directe Acyclic Graph)
					- Transformation
						- Narrow -> Map, Filter, FlatMap, etc..
						- Wide - Repartition, Join, GroupByKey, ReduceByKey, AggregateByKey, Collaesce, etc..
					- Action
						- Count, Sum, Collect, Reduce, ForEach, etc..		

					[1,2,3,4,5] -> 15
					[(2013, [1,1,1]),(2020,[1,1,1,1,])] -> [(2013, 3), (2020, 4)] 
					
				- Spark SQL
				- Spark Streaming
				- Spark MLib
				- Spring GraphX
			

			Transformations
				- Map
				- FlatMap
				- Filter
				- mapPartitions

				- repartition
				- coalesce
				- distinct

				- union
				- intersection
				
				- join
				- cogroup
				- cartesian

				- groupByKey
				- reduceByKey
				- aggregateByKey
				- sortByKey

			Actions
				- count
				- collect
				- reduce
				- sum
				- min
				- max
				- mean
				- forEach
				- saveAsTextFile
				- first
				- take
				- takeSample
				- countByKey
		
			Persistence Level

				- MEMORY_ONLY
				- DISK_ONLY
				- MEMORY_AND_DISK
				- MEMORY_ONLY_2
				- DISK_ONLY_2
				- MEMORY_AND_DISK_2
				
			Share Variables
				Broadcast Variable
				Accumulator

			
	Spark SQL

			Data Frame
			
				- Create DataFram
					- DataFrameReader -> spark.read() -> Load data from external source
					- From RDD -> rdd.toDF()
					- Using CreateDataFrame with RDD -> spark.createDataFrame(rdd)
					- Using CreateDataFrame with List of Rows -> spark.createDataFrame([Row(),Row()]
					- Using CreateDataFrame with List of Tuples -> spark.createDataFrame([(),()]
					- Using CreateDataFrame with Pandas DF -> spark.createDataFrame(pandasDF)
					- Using CreateDataFrame with explicit schema -> spark.createDataFrame(rdd,schema=list|string|structtype)

				- Print Schema -> df.printSchema()
				- List values -> df.show()
				
				- Select columns
					- df.select(df.col, df[‘col’], “col”)


				- Functions
					- String Functions -contcat_ws, lower, upper,
					- Math Functions
					- Date Functions
					- Window Functions
					- etc.

				- Columns
					- Add Column - df.withColumn(‘col_name’, fun())
					- Drop Column
					- Rename Column
				
				- Filtering
				- Grouping
				- Aggregation
				- Joins

				- Write Data From DF
				
				- Create Data Frame with Complex Data Types
					- Struct, Array, Map
				- When Function
				- Expr Function
				- Missing Values Handling - Filter, Drop, Replace
				- Cast Function
				- Col, Lit function
				- Flattening with Explode, Explode Outer, PosExplode, PosExplode Outer
				- Filter
				- GroupBy
				- Array Contains, to_csv()
				- Aggregate Functions
				- Where
				- Distinct
				- OrderBy
				
